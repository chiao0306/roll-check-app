import streamlit as st
from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.ai.documentintelligence.models import AnalyzeResult
import google.generativeai as genai
import json
import concurrent.futures # å¼•å…¥å¹³è¡Œè™•ç†å¥—ä»¶

# --- 1. é é¢è¨­å®š ---
st.set_page_config(page_title="ä¸­é‹¼æ©Ÿæ¢°ç¨½æ ¸", page_icon="ğŸ­", layout="centered")

# --- 2. ç§˜å¯†é‡‘é‘°è®€å– ---
try:
    DOC_ENDPOINT = st.secrets["DOC_ENDPOINT"]
    DOC_KEY = st.secrets["DOC_KEY"]
    GEMINI_KEY = st.secrets["GEMINI_KEY"]
except:
    st.error("æ‰¾ä¸åˆ°é‡‘é‘°ï¼è«‹åœ¨ Streamlit Cloud è¨­å®š Secretsã€‚")
    st.stop()

# --- 3. åˆå§‹åŒ– Session State ---
if 'photo_gallery' not in st.session_state:
    st.session_state.photo_gallery = []
if 'uploader_key' not in st.session_state:
    st.session_state.uploader_key = 0

# --- 4. æ ¸å¿ƒå‡½æ•¸ï¼šAzure ç¥ä¹‹çœ¼ (å–®é è™•ç†é‚è¼¯) ---
def process_single_file(file_data, page_index, endpoint, key):
    """
    é€™æ˜¯ä¸€å€‹ç¨ç«‹å‡½æ•¸ï¼Œå°ˆé–€ç”¨ä¾†è™•ç†å–®å¼µåœ–ç‰‡ï¼Œæ–¹ä¾¿å¹³è¡Œé‹ç®—ã€‚
    """
    try:
        client = DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(key))
        
        # å‘¼å« Azure
        poller = client.begin_analyze_document(
            "prebuilt-layout", 
            file_data,
            content_type="application/octet-stream"
        )
        result: AnalyzeResult = poller.result()
        
        markdown_output = ""
        # æå–è¡¨æ ¼
        if result.tables:
            for idx, table in enumerate(result.tables):
                # å˜—è©¦æŠ“å–é ç¢¼ï¼Œè‹¥æŠ“ä¸åˆ°å°±ç”¨å‚³å…¥çš„ index
                p_num = table.bounding_regions[0].page_number if table.bounding_regions else (page_index + 1)
                
                markdown_output += f"\n### Table {idx + 1} (Page {p_num}):\n"
                rows = {}
                for cell in table.cells:
                    r, c = cell.row_index, cell.column_index
                    content = cell.content.replace("\n", " ").strip()
                    if r not in rows: rows[r] = {}
                    rows[r][c] = content
                for r in sorted(rows.keys()):
                    row_cells = []
                    if rows[r]:
                        max_col = max(rows[r].keys())
                        for c in range(max_col + 1): row_cells.append(rows[r].get(c, ""))
                        markdown_output += "| " + " | ".join(row_cells) + " |\n"
        
        # ã€è³‡æ–™ç˜¦èº«ã€‘ï¼šåªå–å…¨æ–‡çš„å‰ 1000 å€‹å­—å…ƒ (é€šå¸¸è¡¨é ­éƒ½åœ¨æœ€ä¸Šé¢)
        # é€™æ¨£å¯ä»¥å¤§å¹…æ¸›å°‘ Gemini çš„é–±è®€è² æ“”ï¼ŒåŠ å¿«é€Ÿåº¦
        header_text_snippet = result.content[:1000] if result.content else ""
        
        return {
            "page": page_index + 1,
            "table": markdown_output,
            "header_text": header_text_snippet,
            "success": True
        }
    except Exception as e:
        return {"page": page_index + 1, "error": str(e), "success": False}

# --- 5. æ ¸å¿ƒå‡½æ•¸ï¼šGemini ç¥ä¹‹è…¦ ---
def audit_with_gemini(extracted_data_list, api_key):
    genai.configure(api_key=api_key)
    # å …æŒä½¿ç”¨ 2.5 Pro ä»¥ç¢ºä¿ç²¾åº¦ï¼Œé å‰é¢çš„å¹³è¡ŒåŒ–ä¾†è£œé€Ÿåº¦
    model = genai.GenerativeModel("models/gemini-2.5-pro")
    
    combined_input = "ä»¥ä¸‹æ˜¯å„é è³‡æ–™ï¼š\n"
    for data in extracted_data_list:
        combined_input += f"\n--- Page {data['page']} ---\n"
        combined_input += f"ã€é é¦–æ–‡å­—ç‰‡æ®µ (æª¢æŸ¥å·¥ä»¤/æ—¥æœŸ)ã€‘:\n{data['header_text']}\n"
        combined_input += f"ã€è¡¨æ ¼æ•¸æ“šã€‘:\n{data['table']}\n"

    system_prompt = """
    ä½ æ˜¯ä¸€ä½æ¥µåº¦åš´è¬¹çš„ä¸­é‹¼æ©Ÿæ¢°å“ç®¡ç¨½æ ¸å“¡ã€‚
    
    è«‹åŸ·è¡Œä»¥ä¸‹ **å…¨æ–¹ä½é‚è¼¯ç¨½æ ¸**ï¼š

    ### 0. è·¨é ä¸€è‡´æ€§èˆ‡æ ¼å¼æª¢æŸ¥ (Header Consistency)ï¼š
    - **ä¾†æº**ï¼šè«‹å¾ã€Œé é¦–æ–‡å­—ç‰‡æ®µã€ä¸­å°‹æ‰¾è³‡è¨Šã€‚
    - **ç›®æ¨™**ï¼š
      1. **å·¥ä»¤ç·¨è™Ÿ** (Job No)
      2. **é å®šäº¤è²¨æ—¥æœŸ** (Scheduled Date)
      3. **å¯¦éš›äº¤è²¨æ—¥æœŸ** (Actual Date)
    - **è¦å‰‡**ï¼š
      - æ‰€æœ‰é é¢çš„ä¸Šè¿°ä¸‰å€‹æ¬„ä½å…§å®¹å¿…é ˆã€Œå®Œå…¨ç›¸åŒã€ã€‚ä¸åŒ -> **FAIL**ã€‚
      - æ—¥æœŸæ ¼å¼å¿…é ˆç‚º `YYY.MM.DD` (å¦‚ `114.10.30`)ã€‚æ ¼å¼éŒ¯èª¤ -> **FAIL**ã€‚

    ### 1. è£½ç¨‹åˆ¤å®šé‚è¼¯ (Process Logic)ï¼š
    - **æœªå†ç”Ÿ/è»Šä¿®**ï¼šå¯¦æ¸¬å€¼ **<= (å°æ–¼æˆ–ç­‰æ–¼)** è¦æ ¼å€¼ã€‚
    - **éŠ²è£œ (Welding)**ï¼šå¯¦æ¸¬å€¼ **>= (å¤§æ–¼æˆ–ç­‰æ–¼)** è¦æ ¼å€¼ã€‚
    - **å†ç”Ÿè»Šä¿®**ï¼šå¯¦æ¸¬å€¼å¿…é ˆ **åŒ…å«æ–¼ (Inclusive)** ä¸Šä¸‹é™ä¹‹é–“ã€‚

    ### 2. æ•¸é‡ä¸€è‡´æ€§æª¢æŸ¥ (Quantity Check)ï¼š
    - **æ­¥é©Ÿ**ï¼šè®€å–é …ç›®åç¨±ä¸­çš„æ•¸é‡è¦æ±‚ `(10PC)` -> æ¸…é»è©²åˆ—å¯¦æ¸¬æ•¸æ“šå€‹æ•¸ -> æ¯”å°ã€‚
    - **è¦å‰‡**ï¼šè‹¥ `å¯¦æ¸¬å€‹æ•¸ < è¦æ±‚å€‹æ•¸` -> **FAIL (æ•¸é‡ä¸ç¬¦)**ã€‚
    - **ä¾‹å¤–**ï¼šåƒ…ã€Œç†±è™•ç†ã€å¿½ç•¥æ•¸é‡ã€‚

    ### 3. å¤šé‡è¦æ ¼æ™ºæ…§æ­¸é¡ (Multi-Spec Matching)ï¼š
    - è‹¥é …ç›®æœ‰å¤šç¨®å°ºå¯¸è¦æ ¼ï¼ˆå¦‚ï¼šä¸€ã€157mmï¼›äºŒã€127mmï¼‰ã€‚
    - å°æ¯å€‹å¯¦æ¸¬å€¼ï¼Œè‡ªå‹•åˆ¤æ–·å®ƒæ¥è¿‘å“ªä¸€å€‹è¦æ ¼ï¼Œå°±å¥—ç”¨è©²è¦æ ¼çš„åˆ¤å®šæ¨™æº–ã€‚

    ### 4. æ•¸å­¸æ¯”å°åš´è¬¹åº¦ï¼š
    - é€²è¡Œ **å°æ•¸é»å¾Œå…©ä½** çš„ç²¾ç¢ºæ¯”å°ã€‚

    ### è¼¸å‡ºæ ¼å¼ (JSON Only)ï¼š
    {
      "job_no": "å·¥ä»¤ç·¨è™Ÿ",
      "summary": "ç¸½çµç™¼ç¾å¹¾å€‹ç•°å¸¸",
      "issues": [
         {
           "page": 1,
           "item": "é …ç›®åç¨±",
           "spec_logic": "åˆ¤å®šæ¨™æº–",
           "measured": "å¯¦æ¸¬æ•¸æ“š",
           "issue_type": "æ•¸å€¼è¶…è¦ / æ•¸é‡ä¸ç¬¦ / è·¨é è³‡è¨Šä¸ç¬¦ / æ—¥æœŸæ ¼å¼éŒ¯èª¤",
           "reason": "è©³ç´°èªªæ˜"
         }
      ]
    }
    """
    
    try:
        response = model.generate_content(
            [system_prompt, combined_input],
            generation_config={"response_mime_type": "application/json"}
        )
        return response.text
    except Exception as e:
        return f"Error: {str(e)}"

# --- 6. æ‰‹æ©Ÿç‰ˆ UI ---
st.title("ğŸ­ ç¾å ´ç¨½æ ¸åŠ©æ‰‹")

# A. æª”æ¡ˆä¸Šå‚³å€
with st.container(border=True):
    st.subheader("ğŸ“‚ æ–°å¢é é¢")
    uploaded_files = st.file_uploader(
        "é»æ“Šä¸Šå‚³ (æ‰‹æ©Ÿå¯é¸ç›´æ¥æ‹ç…§)", 
        type=['jpg', 'png', 'jpeg'], 
        accept_multiple_files=True,
        key=f"uploader_{st.session_state.uploader_key}"
    )
    if uploaded_files:
        for f in uploaded_files:
            st.session_state.photo_gallery.append(f)
        st.session_state.uploader_key += 1
        st.rerun()

# B. é è¦½èˆ‡ç®¡ç†å€
if st.session_state.photo_gallery:
    st.divider()
    st.write(f"ğŸ“Š å·²ç´¯ç© **{len(st.session_state.photo_gallery)}** é æ–‡ä»¶")
    
    cols = st.columns(3)
    for idx, img in enumerate(st.session_state.photo_gallery):
        with cols[idx % 3]:
            st.image(img, caption=f"P.{idx+1}", use_container_width=True)
            if st.button("âŒ", key=f"del_{idx}"):
                st.session_state.photo_gallery.pop(idx)
                st.rerun()

    # C. åŸ·è¡ŒæŒ‰éˆ•
    st.divider()
    
    if st.button("ğŸš€ æ¥µé€Ÿå¹³è¡Œåˆ†æ (2.5 Pro)", type="primary", use_container_width=True):
        
        status = st.empty()
        progress_bar = st.progress(0)
        
        # 1. å¹³è¡Œè™•ç† Azure OCR
        status.text("Azure æ­£åœ¨å¹³è¡Œæƒææ‰€æœ‰é é¢ (æ¥µé€Ÿæ¨¡å¼)...")
        
        extracted_data_list = []
        # æº–å‚™è¦å‚³å…¥çš„åƒæ•¸åˆ—è¡¨
        files_to_process = []
        for i, img in enumerate(st.session_state.photo_gallery):
            # å¿…é ˆè®€å– bytes æ‰èƒ½å‚³å…¥åŸ·è¡Œç·’
            img.seek(0)
            files_to_process.append((img.read(), i))

        # ã€é—œéµåŠ é€Ÿé»ã€‘ï¼šä½¿ç”¨ ThreadPoolExecutor åŒæ™‚ç™¼é€è«‹æ±‚
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            # æäº¤ä»»å‹™
            future_to_page = {
                executor.submit(process_single_file, f_data, idx, DOC_ENDPOINT, DOC_KEY): idx 
                for f_data, idx in files_to_process
            }
            
            results = []
            for future in concurrent.futures.as_completed(future_to_page):
                data = future.result()
                results.append(data)
                # æ›´æ–°é€²åº¦æ¢
                progress_bar.progress(len(results) / (len(files_to_process) + 1))
        
        # ä¾ç…§é ç¢¼é‡æ–°æ’åº (å› ç‚ºå¹³è¡Œè™•ç†å›ä¾†çš„é †åºæ˜¯ä¸å›ºå®šçš„)
        results.sort(key=lambda x: x['page'])
        
        # æª¢æŸ¥æ˜¯å¦æœ‰ OCR éŒ¯èª¤
        valid_results = [r for r in results if r['success']]
        if len(valid_results) < len(results):
            st.error("éƒ¨åˆ†é é¢è®€å–å¤±æ•—ï¼Œè«‹æª¢æŸ¥ç¶²è·¯æˆ–åœ–ç‰‡ã€‚")
        
        # 2. Gemini æ€è€ƒ
        status.text("Gemini 2.5 Pro æ­£åœ¨é€²è¡Œé‚è¼¯ç¨½æ ¸...")
        result_str = audit_with_gemini(valid_results, GEMINI_KEY)
        
        progress_bar.progress(100)
        status.text("å®Œæˆï¼")

        # 3. é¡¯ç¤ºçµæœ
        try:
            result = json.loads(result_str)
            if isinstance(result, list): result = result[0] if len(result) > 0 else {}
            
            st.success(f"å·¥ä»¤: {result.get('job_no', 'Unknown')}")
            
            issues = result.get('issues', [])
            if not issues:
                st.balloons()
                st.success("âœ… å…¨æ•¸åˆæ ¼ï¼")
            else:
                st.error(f"ç™¼ç¾ {len(issues)} å€‹ç•°å¸¸")
                for item in issues:
                    with st.container(border=True):
                        st.markdown(f"**{item.get('item')}**")
                        st.write(f"ğŸš« `{item.get('issue_type')}`")
                        st.caption(f"å¯¦æ¸¬/å…§å®¹: {item.get('measured')}")
                        st.caption(f"åŸå› : {item.get('reason')}")
        except:
            st.error("åˆ†æéŒ¯èª¤")
            st.code(result_str)
            
    if st.button("ğŸ—‘ï¸ æ¸…é™¤æ‰€æœ‰ç…§ç‰‡"):
        st.session_state.photo_gallery = []
        st.rerun()

else:
    st.info("ğŸ‘† è«‹é»æ“Šä¸Šæ–¹æŒ‰éˆ•é–‹å§‹æ–°å¢ç…§ç‰‡")
